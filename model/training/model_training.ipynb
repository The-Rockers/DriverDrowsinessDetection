{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.append(\"model/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_data import Image, DriverDrowsinessDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, classes, class_true, class_pred=None,smooth=True):\n",
    "    assert len(images) == len(class_true)\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    if class_pred is None:\n",
    "        hspace = 0.3\n",
    "    else:\n",
    "        hspace = 0.6\n",
    "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
    "    if smooth:\n",
    "        interpolation = 'spline16'\n",
    "    else:\n",
    "        interpolation = 'nearest'\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(images):\n",
    "            ax.imshow(images[i], interpolation=interpolation)\n",
    "            cls_true_name = classes[class_true[i]]\n",
    "            if class_pred is None:\n",
    "                xlabel = \"True: {0}\".format(class_true[i])\n",
    "            else:\n",
    "                class_pred_name = classes[class_pred[i]]\n",
    "                xlabel = \"True: {0}\\nPred: {1}\".format(cls_true_name, class_pred_name)\n",
    "            ax.set_xlabel(xlabel)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "def print_confusion_matrix(classes,class_test,class_pred):\n",
    "    \"\"\"\n",
    "    prints the confusion matrix. class_pred is the array of all predicted classes of each image.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true=class_test, y_pred=class_pred)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(cm)\n",
    "    for i, class_name in enumerate(classes):\n",
    "        print(\"({0}) {1}\".format(i, class_name))\n",
    "\n",
    "def plot_training_history(model):\n",
    "    # Get the classification accuracy and loss-value\n",
    "    # for the training-set.\n",
    "    acc = model.history['accuracy']\n",
    "    loss = model.history['loss']\n",
    "\n",
    "    # Get it for the validation-set (we only use the test-set).\n",
    "    val_acc = model.history['val_accuracy']\n",
    "    val_loss = model.history['val_loss']\n",
    "\n",
    "    # Plot the accuracy and loss-values for the training-set.\n",
    "    plt.plot(acc, linestyle='-', color='b', label='Training Acc.')\n",
    "    plt.plot(loss, 'o', color='b', label='Training Loss')\n",
    "    \n",
    "    # Plot it for the test-set.\n",
    "    plt.plot(val_acc, linestyle='--', color='r', label='Test Acc.')\n",
    "    plt.plot(val_loss, 'o', color='r', label='Test Loss')\n",
    "\n",
    "    # Plot title and legend.\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Ensure the plot shows correctly.\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 0, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIMS = (224,224,3)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "with open(\"../data/data.pkl\",\"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "frames,labels = zip(*dataset.values())\n",
    "\n",
    "# preprocess all frames\n",
    "frames = np.array([Image.load_and_prep_image(frame,dimensions=(DIMS[0],DIMS[1])) for frame in frames])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(frames,labels,test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val = np.array(X_train), np.array(X_val)\n",
    "y_train, y_val = np.array(y_train), np.array(y_val)\n",
    "\n",
    "def data_generator(X,y,batch_size):\n",
    "    num_samples = len(X)\n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "            X_batch, y_batch = X[batch_indices], y[batch_indices]\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "    log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "    return tensorboard_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_generator(X_train, y_train, BATCH_SIZE)\n",
    "val_generator = data_generator(X_val, y_val, BATCH_SIZE)\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=DIMS),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "    layers.experimental.preprocessing.RandomHeight(0.2),\n",
    "    layers.experimental.preprocessing.RandomWidth(0.2),\n",
    "], name=\"data_augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = tf.keras.applications.EfficientNetV2B0(\n",
    "    input_shape=DIMS,\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    ")\n",
    "\n",
    "def EfficientNet(input_shape=DIMS,base_model=BASE_MODEL, num_classes=2):\n",
    "    # freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # create input layer\n",
    "    inputs = keras.Input(shape=input_shape, name=\"input_layer\")\n",
    "    x = data_augmentation(inputs)\n",
    "    x = base_model(x, training=False)\n",
    "    x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\", name=\"dense_layer\")(x)\n",
    "    x = layers.Dropout(0.7, name=\"dropout_layer\")(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"sigmoid\", name=\"output_layer\")(x)\n",
    "\n",
    "\n",
    "    # create a new model with the EfficientNetV2B0 base model and a GlobalAveragePooling2D layer. assess model performance with metrics such as accuracy, loss, and f1 score\n",
    "    model = keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\",keras.metrics.Precision(),keras.metrics.Recall(),keras.metrics.AUC()]\n",
    "    )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[3].dtype)\n",
    "print(model.layers[3].dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(train_generator,\n",
    "                  epochs=10,\n",
    "                  steps_per_epoch=len(X_train)//BATCH_SIZE,validation_data=val_generator,\n",
    "                  validation_steps=len(X_val)//BATCH_SIZE,\n",
    "                  callbacks=[create_tensorboard_callback(\"logs\",\"EfficientNetV2B0\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"en_model_v1.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
